{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Querying of data in S3 with Athena and Generative AI (Text-to-SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this sample notebook, you'll see how generative AI can simplify the process of querying and analyzing data stored in Amazon S3 using AWS Athena and the Glue Catalog. Instead of manually writing complex SQL queries, we'll showcase how to describe your analysis requirements in plain English text, and leverage a Generative AI model to generate the corresponding Athena SQL queries automatically.\n",
    "\n",
    "Athena is an interactive query service that enables analysts to analyze data in S3 using standard SQL. However, constructing SQL queries, especially for complex analysis requirements, can be challenging. This is where the Glue Catalog can help - it stores table definitions and schemas for your data in S3, allowing Athena to query that data seamlessly.\n",
    "\n",
    "This notebook illustrates how introducing generative AI can bridge the gap. \n",
    "\n",
    "1. Overview of text-to-SQL capabilities using GenAI models\n",
    "2. Utilizing the Glue Catalog table definitions\n",
    "3. Generating and executing Athena SQL queries from natural language descriptions\n",
    "4. Using Generative AI for self correcting failed queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need the following to execute the code in this notebook:\n",
    "\n",
    "- Access to Athena (for query execution), S3 (read access), Glue Catalog (For Database and Tables) and Bedrock (LLMs).\n",
    "- Claude 3 Sonnet Model Enabled on Bedrock. You can read more about model access here: https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n",
    "- An S3 bucket to store the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "# Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"text-to-sql-architecture.png\" width=\"800\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture flow:\n",
    "\n",
    "1. Create the AWS Glue Data Catalog using the AWS SDK or an AWS Glue crawler. (In this example, we will use the [AWS SDK for Pandas Library](https://github.com/awslabs/aws-data-wrangler)))\n",
    "\n",
    "2. Use the Titan-Text-Embeddings model on Amazon Bedrock to convert the metadata into embeddings and store them in a vector store, which serves as our knowledge base in the Retrieval Augmented Generation (RAG) framework. (In this example, we use FAISS as our vector store via Langchain. Alternatively, you can use OpenSearch for a vector database. Learn more about OpenSearch Vector Database Capabilities [here](https://aws.amazon.com/blogs/big-data/amazon-opensearch-services-vector-database-capabilities-explained/))\n",
    "\n",
    "3. The user enters their query in natural language.\n",
    "\n",
    "4. Fetch relevant context (relevant tables) from the vector store based on the user's query.\n",
    "\n",
    "5. Pass the user's query along with the relevant tables (context) to the Claude 3 model to generate a SQL query. This technique of retrieving context and passing it along with the question to the model is called Retrieval Augmented Generation (RAG).\n",
    "\n",
    "6. Execute the SQL query generated by the model using Amazon Athena.\n",
    "\n",
    "7. If Athena returns an error message (possibly due to an incorrect SQL query), proceed to the correction loop (Steps 8-9).\n",
    "\n",
    "8. Pass the error message from Athena and the incorrect SQL query generated to the Large Language Model (LLM) to correct it.\n",
    "\n",
    "9. The LLM creates the corrected SQL query. This iteration can be performed multiple times if needed.\n",
    "\n",
    "10. Finally, run the corrected SQL query using Athena and present the output to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install awswrangler pandas boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "Data Preparation and Exploration are crucial aspects of any Generative AI application. For this Text-to-SQL workshop we will begin with loading of sample data files and creating a data catalog as they lay the foundation for effective querying and extracting insights from data using SQL.\n",
    "\n",
    "We will Start by installing required Python libraries AWS SDK for Pandas (awswrangler), Pandas and Boto3. AWS SDK for Pandas (awswrangler) is a Python library that simplifies the interaction between Python and the AWS ecosystem, providing a high-level API for working with a wide range of AWS services, including Amazon S3, Athena, Glue, Redshift, and DynamoDB. The library abstracts the complexity of AWS service integrations, streamlines common data engineering tasks, and integrates with other popular data science and machine learning libraries, making it a valuable tool for developers and data professionals working on AWS-based projects. For more information and references, visit [AWS Wrangler GitHub repository](https://github.com/awslabs/aws-data-wrangler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create Database and Tables in Glue Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use AWS SDK for Pandas (awswrangler) library to interact with Glue Data Catalog and retrieve a list of all databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# List all databases in the Glue Data Catalog\n",
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below sample code reads the customer csv file, creates a new database and table in the Glue Data Catalog, writes the data to an S3 location as a Parquet dataset, and then retrieves the first 10 rows of the table using an Athena SQL query.\n",
    "\n",
    "Make sure you replace the \"<BUCKET_NAME>\" with the name of the bucket in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create catalog database: workshop_test\n",
    "# Create first table (customers) in the database\n",
    "\n",
    "bucket = \"<BUCKET_NAME>\"\n",
    "path = f\"s3://{bucket}/data/\"\n",
    "\n",
    "# Read local csv file customers.csv in to a DataFrame\n",
    "df = pd.read_csv(\"../dataset/customers.csv\")\n",
    "\n",
    "# Check if the database workshop_test exists\n",
    "if \"workshop_test\" not in databases.values:\n",
    "    wr.catalog.create_database(\"workshop_test\")\n",
    "    print(wr.catalog.databases())\n",
    "else:\n",
    "    print(\"Database workshop_test already exists\")\n",
    "\n",
    "# List all tables in the database workshop_test\n",
    "dbs = wr.catalog.tables(database=\"workshop_test\")\n",
    "# print table count\n",
    "\n",
    "print(\"There are {} tables in the database workshop_test\".format(len(dbs)))\n",
    "\n",
    "\n",
    "print(\"Creating table customers in the database workshop_test\")\n",
    "# Create table customers in the database workshop_test\n",
    "desc = \"Table with list of customers\"\n",
    "param = {\"source\": \"Customer Details Table\", \"class\": \"e-commerce\"}\n",
    "\n",
    "comments = {\n",
    "    \"customer_id\": \"Unique customer ID.\",\n",
    "    \"first_name\": \"Customer first name.\",\n",
    "    \"last_name\": \"Customer last name.\",\n",
    "    \"email_id\": \"Customer email ID.\",\n",
    "    \"phone_num\": \"Customer phone number.\",\n",
    "}\n",
    "\n",
    "res = wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=f\"s3://{bucket}/customers/\",\n",
    "    dataset=True,\n",
    "    database=\"workshop_test\",\n",
    "    table=\"customers\",\n",
    "    mode=\"overwrite\",\n",
    "    glue_table_settings=wr.typing.GlueTableSettings(description=desc, parameters=param, columns_comments=comments),\n",
    ")\n",
    "\n",
    "print(\"Table customers created in the database workshop_test\")\n",
    "\n",
    "\n",
    "# Read table customers from the database workshop_test\n",
    "table = wr.catalog.table(database=\"workshop_test\", table=\"customers\")\n",
    "print(table)\n",
    "\n",
    "\n",
    "print(\"Records in the table customers\")\n",
    "\n",
    "# Run a sample query on the table customers\n",
    "df = wr.athena.read_sql_query(\"SELECT * FROM customers LIMIT 10\", database=\"workshop_test\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables for all the csv files in the dataset folder\n",
    "\n",
    "file_names = [\"orders.csv\", \"products.csv\", \"orderdetails.csv\", \"payments.csv\", \"shipments.csv\", \"reviews.csv\"]\n",
    "\n",
    "comments_dict = {\n",
    "    \"customers\": {\n",
    "        \"customer_id\": \"Unique customer ID.\",\n",
    "        \"first_name\": \"Customer first name.\",\n",
    "        \"last_name\": \"Customer last name.\",\n",
    "        \"email_id\": \"Customer email ID.\",\n",
    "        \"phone_num\": \"Customer phone number.\",\n",
    "    },\n",
    "    \"orderdetails\": {\n",
    "        \"orderdetailid\": \"Unique order detail ID.\",\n",
    "        \"orderid\": \"Unique order ID.\",\n",
    "        \"productid\": \"Unique product ID.\",\n",
    "        \"quantity\": \"Quantity of product ordered.\",\n",
    "        \"price\": \"Price of product.\",\n",
    "    },\n",
    "    \"orders\": {\n",
    "        \"orderid\": \"Unique order ID.\",\n",
    "        \"customerid\": \"Unique customer ID.\",\n",
    "        \"orderdate\": \"Order date.\",\n",
    "        \"totalamount\": \"Total order amount.\",\n",
    "    },\n",
    "    \"payments\": {\n",
    "        \"paymentid\": \"Unique payment ID.\",\n",
    "        \"orderid\": \"Unique order ID.\",\n",
    "        \"paymenttype\": \"Type of payment.\",\n",
    "        \"amount\": \"Payment amount.\",\n",
    "        \"paymentdate\": \"Payment date.\",\n",
    "        \"status\": \"Payment status.\",\n",
    "    },\n",
    "    \"products\": {\n",
    "        \"productid\": \"Unique product ID.\",\n",
    "        \"productname\": \"Product name.\",\n",
    "        \"price\": \"Product price.\",\n",
    "        \"category\": \"Product category.\",\n",
    "        \"stock\": \"Product stock.\",\n",
    "    },\n",
    "    \"reviews\": {\n",
    "        \"reviewid\": \"Unique review ID.\",\n",
    "        \"productid\": \"Unique product ID.\",\n",
    "        \"customerid\": \"Unique customer ID.\",\n",
    "        \"rating\": \"Product rating.\",\n",
    "        \"comment\": \"Review comment.\",\n",
    "        \"reviewdate\": \"Review date.\",\n",
    "    },\n",
    "    \"shipments\": {\n",
    "        \"shipmentid\": \"Unique shipment ID.\",\n",
    "        \"orderid\": \"Unique order ID.\",\n",
    "        \"status\": \"Shipment status.\",\n",
    "        \"estimateddelivery\": \"Estimated delivery date.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Creating tables for all the csv files in the dataset folder\")\n",
    "\n",
    "for file_name in file_names:\n",
    "    table_name = file_name.split(\".\")[0]\n",
    "    df = pd.read_csv(f\"../dataset/{file_name}\")\n",
    "    res = wr.s3.to_parquet(\n",
    "        df=df,\n",
    "        path=f\"s3://{bucket}/{table_name}/\",\n",
    "        dataset=True,\n",
    "        database=\"workshop_test\",\n",
    "        table=table_name,\n",
    "        mode=\"overwrite\",\n",
    "        glue_table_settings=wr.typing.GlueTableSettings(\n",
    "            description=f\"Table with list of {table_name}.\",\n",
    "            parameters={\"source\": f\"{table_name} Table\", \"class\": \"e-commerce\"},\n",
    "            columns_comments=comments_dict[table_name],\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Table {table_name} created in the database workshop_test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Check created tables and retrieve schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# New Database in the Glue Data Catalog\n",
    "database = \"workshop_test\"\n",
    "\n",
    "# List all tables in the database workshop_test\n",
    "all_tables = wr.catalog.tables(database=database)\n",
    "\n",
    "# Get list of all table names\n",
    "tables = all_tables[\"Table\"].tolist()\n",
    "\n",
    "print(\"List of all tables in the database workshop_test\")\n",
    "print(tables)\n",
    "\n",
    "\n",
    "# Get schema for all tables\n",
    "print(\"Schema for all tables in the database workshop_test\")\n",
    "\n",
    "all_schemas = {}\n",
    "for table in tables:\n",
    "    schema_str = wr.catalog.get_table_types(database=database, table=table)\n",
    "    all_schemas[table] = schema_str\n",
    "\n",
    "pprint.pprint(all_schemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate data load by fetching top 2 records from each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = wr.catalog.tables(database=database)\n",
    "tables = all_tables[\"Table\"].tolist()\n",
    "print(tables)\n",
    "\n",
    "for table in tables:\n",
    "    data = wr.athena.read_sql_query(f\"SELECT * FROM {table} LIMIT 10\", database=\"workshop_test\")\n",
    "    # print first 2 rows of the data\n",
    "    \n",
    "    print(f\"First 2 rows of the table {table}\")\n",
    "    print(data.head(2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Generative AI using Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. With Bedrock's serverless experience, you can get started quickly, privately customize FMs with your own data, and easily integrate and deploy them into your applications using the AWS tools without having to manage any infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Bedrock Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a boto3 bedrock client. We can use this client to issue API calls to Generative AI models available in Bedrock.\n",
    "\n",
    "Note: You can replace the profile_name with the profile name that is configured on your developer environment that has access to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from botocore.config import Config\n",
    "import json\n",
    "retry_config = Config(\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "session = boto3.Session(region_name='us-east-1', profile_name='default')\n",
    "bedrock_client = session.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        config=retry_config\n",
    "    )\n",
    "\n",
    "print(\"boto3 Bedrock client successfully created!\")\n",
    "\n",
    "############################\n",
    "# Note: You can also create a boto3 session with the credentials from environment variables\n",
    "############################\n",
    "\n",
    "# # Get the AWS credentials from environment variables\n",
    "# AWS_ACCESS_KEY_ID = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "# AWS_SECRET_ACCESS_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "# AWS_SESSION_TOKEN = os.environ.get('AWS_SESSION_TOKEN', None)  # Optional, if using temporary credentials\n",
    "\n",
    "# # Create a boto3 session with the credentials from environment variables\n",
    "# session = boto3.Session(\n",
    "#     aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "#     aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "#     aws_session_token=AWS_SESSION_TOKEN,  # Include this line only if using temporary credentials\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Creating a function to call Bedrock API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a resuable function that uses the client we created above to call Claude 3 Sonnet model on Bedrock. We can pass prompt and temperature to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock_claude_3(prompt_text, temperature):\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt_text\n",
    "                        }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    body = json.dumps(body)\n",
    "    response = bedrock_client.invoke_model(\n",
    "            body=body, modelId=model_id\n",
    "        )\n",
    "    # Parse the response\n",
    "    response_lines = response['body'].readlines()\n",
    "    json_str = response_lines[0].decode('utf-8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    result_text = json_obj['content'][0]['text']\n",
    "    \n",
    "    return result_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test a sample bedrock call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Claude 3 Sonnet model\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "prompt = \"Hello...\"\n",
    "\n",
    "# Call Bedrock and get the response\n",
    "response = call_bedrock_claude_3(prompt, 0.7)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Text to SQL with Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll showcase how to use generative AI models like Claude 3 to automatically generate SQL queries for analyzing data in Amazon S3 using Athena. We'll provide the AI model with the table schemas from the Glue Catalog and a natural language question describing the desired analysis. The model will then generate the corresponding Athena SQL query, which we can execute to retrieve the results. We'll also build an interactive chat widget that allows you to enter questions in plain English and see the AI-generated SQL queries and their output displayed interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Testing SQL generation and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a schemas dict which has list of all tables and corresponding columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "database = \"workshop_test\"\n",
    "\n",
    "# List all tables in the database workshop_test\n",
    "all_tables = wr.catalog.tables(database=database)\n",
    "\n",
    "tables = all_tables[\"Table\"].tolist()\n",
    "\n",
    "print(\"List of all tables in the database workshop_test\")\n",
    "print(tables)\n",
    "\n",
    "\n",
    "# Get schema for all tables\n",
    "print(\"Schema for all tables in the database workshop_test\")\n",
    "\n",
    "all_schemas = {}\n",
    "for table in tables:\n",
    "    schema_str = wr.catalog.get_table_types(database=database, table=table)\n",
    "    all_schemas[table] = schema_str\n",
    "\n",
    "pprint.pprint(all_schemas)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a simple prompt with placeholders for tables (schemas) and question. We will then format the prompt by inserting table schemas and question and invoke Claude model on Bedrock to generate SQL query. Finally, we will execute the SQL query using Athena.\n",
    "\n",
    "Note: We will use call_bedrock function created in the previous cells to invoke the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SQL generation prompt\n",
    "prompt = \"\"\"Given the following list of tables, generate syntactically correct SQL query to answer the following question. \\n\\n\n",
    "Tables: \\n\n",
    "{tables} \\n\\n\n",
    "\n",
    "Question: \\n\n",
    "{question} \\n\\n\n",
    "\n",
    "Strict Instructions: \\n\n",
    "- Always end with a semicolon. \\n\\n\n",
    "- Only include the SQL query in the response. \\n\\n\n",
    "- Always name all columns in the query. \\n\\n\n",
    "SQL:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"What is the total number of customers?\"\n",
    "\n",
    "# Format the prompt\n",
    "prompt = prompt.format(tables=all_schemas, question=question)\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "# Call Bedrock and get the response\n",
    "sql = call_bedrock_claude_3(prompt, 0.7)\n",
    "print(sql)\n",
    "\n",
    "# Run the generated SQL query\n",
    "df = wr.athena.read_sql_query(sql, database=database)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Creating a text-to-sql chat widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a simple text-to-sql chat widget that can run within a notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Create text output widget for displaying messages\n",
    "output = widgets.Output()\n",
    "base_prompt = \"\"\"Given the following list of tables, generate syntactically correct SQL query to answer the following question. \\n\\n\n",
    "Tables: \\n\n",
    "{tables} \\n\\n\n",
    "Question: \\n\n",
    "{question} \\n\\n\n",
    "\n",
    "Strict Instructions: \\n\n",
    "- Always end with a semicolon. \\n\\n\n",
    "- Only include the SQL query in the response. \\n\\n\n",
    "- Always name all columns in the query. \\n\\n\n",
    "SQL:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Create a text input widget\n",
    "input_text = widgets.Text(\n",
    "    placeholder='Type your question here...',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a button widget for sending messages\n",
    "send_button = widgets.Button(\n",
    "    description='Send',\n",
    "    button_style='',\n",
    "    tooltip='Click to send your message',\n",
    "    icon='paper-plane'\n",
    ")\n",
    "\n",
    "# Create a button widget for clearing output\n",
    "clear_button = widgets.Button(\n",
    "    description='Clear',\n",
    "    button_style='',\n",
    "    tooltip='Clear output',\n",
    "    icon='trash'\n",
    ")\n",
    "\n",
    "# Create a label for displaying loading status\n",
    "loading_label = widgets.Label(value='')\n",
    "\n",
    "# Function to handle sending message when button is clicked\n",
    "def on_send_button_clicked(b):\n",
    "    global base_prompt\n",
    "    global model_id\n",
    "    global all_schemas\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        loading_label.value = 'Processing...'\n",
    "    display(loading_label)\n",
    "    prompt = base_prompt.format(tables=all_schemas, question=input_text.value)  # Replace 'Your_Tables_Here' with actual table list\n",
    "    sql = call_bedrock_claude_3(prompt, 0.7)\n",
    "    print(sql)\n",
    "    df =  wr.athena.read_sql_query(sql, database=\"workshop_test\")\n",
    "    with output:\n",
    "        display(df)\n",
    "        display(sql)\n",
    "    input_text.value = ''  # Clear the input field after sending\n",
    "    loading_label.value = ''\n",
    "\n",
    "# Function to clear the output\n",
    "def on_clear_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "    loading_label.value = ''\n",
    "\n",
    "# Bind the buttons to their respective functions\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(input_text, send_button, clear_button, output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Using RAG to fetch table metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we will explore the use of the Retrieval Augmented Generation (RAG) approach to efficiently fetch relevant table metadata from a database based on a user's natural language query. This approach leverages the power of vector databases and language model embeddings to retrieve only the most relevant tables and their associated metadata, rather than processing and passing all table metadata for every query.\n",
    "\n",
    "The process begins by hydrating a vector database with the table metadata fetched from the AWS Glue Catalog. This metadata includes table names, descriptions, and schemas. By using natural language embeddings and similarity searches, the RAG approach can understand the context of the user's question and retrieve the most relevant tables from the vector database.\n",
    "\n",
    "This approach offers several advantages over traditional methods:\n",
    "\n",
    "1. **Efficiency**: Instead of processing all table metadata for every query, the RAG approach only works with the relevant subset of tables, reducing computational overhead and improving response times.\n",
    "\n",
    "2. **Scalability**: As the number of tables in the database grows, the RAG approach remains efficient, as the retrieval process is not affected by the total number of tables.\n",
    "\n",
    "3. **Context-awareness**: By leveraging natural language embeddings and similarity searches, the RAG approach can understand the context of the question and retrieve relevant tables, even if the table names or descriptions do not exactly match the query's wording.\n",
    "\n",
    "4. **Flexibility**: This approach can be applied to various scenarios where specific information needs to be extracted or queried from large data sources, enabling more accurate and context-aware responses.\n",
    "\n",
    "By combining language models, vector databases, and targeted retrieval techniques like RAG, this next section demonstrates a powerful and scalable solution for intelligent data exploration and querying, particularly in scenarios with large and complex data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use FAISS as our vector store and langchain framework to create and hydrate the vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code first retrieves list of all tables in the database, and then creates a dictionary that maps the table names to their descriptions.It then iterates over the list of tables, and for each table, it retrieves the schema and adds it to a dictionary `all_schemas`, which also includes the table name and description for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "database = \"workshop_test\"\n",
    "\n",
    "# List all tables in the database workshop_test\n",
    "all_tables = wr.catalog.tables(database=database)\n",
    "\n",
    "# Get list of all table names and descriptions and create a dictionary\n",
    "# format: {table_name: table_description}\n",
    "tables = all_tables[\"Table\"].tolist()\n",
    "table_desc = all_tables[\"Description\"].tolist()\n",
    "table_dict = dict(zip(tables, table_desc))\n",
    "\n",
    "print(table_dict)\n",
    "\n",
    "# Get schema for all tables\n",
    "\n",
    "all_schemas = {}\n",
    "for table in tables:\n",
    "    schema_str = wr.catalog.get_table_types(database=database, table=table)\n",
    "    all_schemas[table] = {\"table_description\": table_dict[table], \"table_name\": table, \"schema\": schema_str}\n",
    "\n",
    "pprint.pprint(all_schemas)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use Amazon Titan Embeddings model to vectorize each table (and it's schema) and store it in FAISS Vector Database. We will use Langchain Framework to make the code simpler. We will then use similarity_search function to retrive relevant tables for a given natural language question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.docstore.document import Document\n",
    "\n",
    "# Create BedrockEmbeddings object\n",
    "embeddings = BedrockEmbeddings()\n",
    "\n",
    "#create document from schemas. Each document represents a table schema\n",
    "documents = []\n",
    "for table, schema in all_schemas.items():\n",
    "    doc = Document(page_content=str(schema), metadata={\"table\": table})\n",
    "    documents.append(doc)\n",
    "\n",
    "print(documents)\n",
    "\n",
    "# Create FAISS index from the documents\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "print(db.index.ntotal)\n",
    "\n",
    "# Search for tables relevant to the query \"total number of orders\"\n",
    "relevant_tables = db.similarity_search(\"total number of orders\")\n",
    "print(relevant_tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### SQL Generation with selective metadata fetched using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same prompt as before, but we will now only pass the relevant table schemas to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SQL generation prompt\n",
    "prompt = \"\"\"Given the following list of tables, generate syntactically correct SQL query to answer the following question. \\n\\n\n",
    "Tables: \\n\n",
    "{tables} \\n\\n\n",
    "Question: \\n\n",
    "{question} \\n\\n\n",
    "\n",
    "Strict Instructions: \\n\n",
    "- Always end with a semicolon. \\n\\n\n",
    "- Only include the SQL query in the response. \\n\\n\n",
    "- Always name all columns in the query. \\n\\n\n",
    "SQL:\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the total number of orders per customer?\"\n",
    "\n",
    "# Search for relevant tables based on the question\n",
    "sim_search = db.similarity_search(question)\n",
    "\n",
    "print(\"Relevant tables for the question: \", sim_search)\n",
    "\n",
    "tables_str = \"\"\n",
    "\n",
    "# Get table names and schema for the relevant tables\n",
    "for table in sim_search:\n",
    "    tables_str += table.page_content + \"\\n\"\n",
    "\n",
    "print(tables_str)\n",
    "\n",
    "# Format the prompt\n",
    "prompt = prompt.format(tables=tables_str, question=question)\n",
    "\n",
    "# Call Bedrock and get the response\n",
    "sql = call_bedrock_claude_3(prompt, 0.7)\n",
    "print(sql)\n",
    "\n",
    "# Run the generated SQL query\n",
    "df = wr.athena.read_sql_query(sql, database=database)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Self Correction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a text-to-SQL system powered by generative AI, a self-correction loop can be implemented to iteratively correct/refine the generated SQL queries. The process begins with the user providing a natural language request, which the AI model translates into an initial SQL query. The query is then executed against the target database, and the results are evaluated. If the query is unsuccessful or the results do not match the user's intent, the system enters a feedback loop, analyzing the failure mode and using it to update the AI model. The refined query is then re-executed, and the loop continues until a satisfactory result is achieved. This self-correcting mechanism allows the system to learn from its mistakes, improving the quality and accuracy of the generated SQL over time, while also incorporating user feedback to better align the output with their specific needs and the target database schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query with wrong column name\n",
    "incorrect_sql = \"SELECT firstname FROM customers\"\n",
    "try:\n",
    "    results = wr.athena.read_sql_query(incorrect_sql, database=database)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Auto correction and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, we will create a new prompt which instructs the model to correct a failing query. We pass the list of tables, the failing query and error returned by Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"List all the first names of the customers.\"\n",
    "\n",
    "# SQL correction prompt\n",
    "correction_prompt = \"\"\"\n",
    "Given an incorrect SQL query, error code, table information and a question, generate a syntactically correct SQL query to answer the question.\n",
    "\n",
    "Incorrect SQL Query:\n",
    "{incorrect_sql}\n",
    "\n",
    "Tables:\n",
    "{tables}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Error Code:\n",
    "{error_code}\n",
    "\n",
    "Strict Instructions:\n",
    "- Always end with a semicolon.\n",
    "- Only include the SQL query in the response.\n",
    "- Always name all columns in the query.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Run the incorrect SQL query\n",
    "    results = wr.athena.read_sql_query(incorrect_sql, database=database)\n",
    "except Exception as e:\n",
    "    # Handle the error\n",
    "    print(e)\n",
    "\n",
    "    # Error code returned by Athena\n",
    "    error_code = str(e)\n",
    "\n",
    "    # Search for relevant tables based on the question\n",
    "    sim_search = db.similarity_search(question)\n",
    "    tables_str = \"\"\n",
    "\n",
    "    # Get table names and schema for the relevant tables\n",
    "    for table in sim_search:\n",
    "        tables_str += table.page_content + \"\\n\"\n",
    "\n",
    "\n",
    "    # Format the SQL correction prompt\n",
    "    prompt = correction_prompt.format(incorrect_sql=incorrect_sql, tables=tables_str, question=question, error_code=error_code)\n",
    "\n",
    "    # Call Bedrock and get the corrected SQL query\n",
    "    sql = call_bedrock_claude_3(prompt, 0.7)\n",
    "    print(sql)\n",
    "\n",
    "    # Run the corrected SQL query\n",
    "    df = wr.athena.read_sql_query(sql, database=database)\n",
    "    print(df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative AI models have demonstrated remarkable capabilities in understanding natural language and generating structured outputs like SQL queries. This text-to-SQL capability leverages large language models trained on vast amounts of data, including code repositories, documentation, and query logs, to learn the mappings between natural language descriptions and their corresponding SQL representations. By processing human-readable inputs, such as analysis requirements or data exploration prompts, these models can generate SQL queries that accurately capture the intended data operations. This not only simplifies the querying process for non-technical users but also enhances productivity for data professionals by automating the translation from natural language to SQL. Additionally, generative AI models can iteratively refine and self-correct queries based on feedback, enabling more accurate and efficient data interactions. The text-to-SQL capability of generative AI has the potential to democratize data access, improve data literacy, and accelerate data-driven decision-making across various industries and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
